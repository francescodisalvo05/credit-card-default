{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.pipeline import Pipeline  as imb_pipeline\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, auc, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,roc_curve\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/default-of-credit-card-clients.csv')\n",
    "df = df.rename(columns={\"PAY_0\": \"PAY_1\"}, errors=\"raise\")\n",
    "df.drop(columns=['ID'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group unkown education elements with \"Others\", as well as marriage\n",
    "df[\"EDUCATION\"] = df[\"EDUCATION\"].apply(lambda x : x if x < 4 and x > 0 else 4)\n",
    "df[\"MARRIAGE\"] = df[\"MARRIAGE\"].apply(lambda x : x if x < 3 and x > 0 else 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y, random_state=42):\n",
    "    \"\"\"Splits the dataset into train and test set\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25,\n",
    "                                                        stratify= y, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def outliers_lof(X,y, n_neighbors=20):\n",
    "    \"\"\"Removes the outliers by LocalOutlierFactor\"\"\"\n",
    "    y = np.asarray(y)\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    outliers = LocalOutlierFactor(n_neighbors = n_neighbors, n_jobs = -1).fit_predict(X)\n",
    "    X = X[outliers == 1, :]\n",
    "    y = y[outliers == 1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def make_tests(outliers_options,scaling,pca_options,resampling_options):\n",
    "    \"\"\"Defines all the test that must be done\"\"\"\n",
    "    \n",
    "    tests = []\n",
    "    for o in outliers_options:\n",
    "        for s in scaling:\n",
    "            for p in pca_options:\n",
    "                for r in resampling_options:\n",
    "                    tests.append([o,s,p,r])\n",
    "    \n",
    "    return tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"rf\" : {\n",
    "        \"clf__n_estimators\":[80,100,120],\n",
    "        \"clf__criterion\":[\"gini\", \"entropy\"],\n",
    "        \"clf__max_features\":['sqrt','log2'],\n",
    "        \"clf__n_jobs\": [-1],\n",
    "    },\n",
    "    \"svc\" : {\n",
    "        \"clf__kernel\": ['linear', 'rbf', 'poly'],\n",
    "        \"clf__gamma\" :['scale', 'auto'],\n",
    "        \"clf__C\": [0.8, 1.0,1.2] \n",
    "    },\n",
    "    \"knn\" : {\n",
    "        \"clf__n_neighbors\": [10, 50, 75],\n",
    "        \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "        \"clf__algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"],\n",
    "        \"clf__n_jobs\": [-1],\n",
    "    },\n",
    "    \"logisticregression\" : {\n",
    "        \"clf__penalty\": ['l1','l2'],\n",
    "        \"clf__tol\": [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "        \"clf__C\": [0.6,0.8, 1.0,1.2, 1.4],\n",
    "        \"clf__solver\" : ['saga'],\n",
    "        \"clf__max_iter\" : [500]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"rf\" : RandomForestClassifier(random_state=random_state),\n",
    "    \"knn\" : KNeighborsClassifier(n_neighbors=9),\n",
    "    \"svc\" : SVC(),\n",
    "    \"logistic\": LogisticRegression(random_state=random_state)\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline(df, tests, model, param_grid, random_state=42):\n",
    "    \"\"\"Tests all the combinations of Preprocessing and Hyperparameters with \n",
    "    K Fold Cross Validation. Then, test on the test set\"\"\"\n",
    "    \n",
    "    \n",
    "    y, X = df[\"default payment next month\"], df.drop(\"default payment next month\", axis=1)\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y, 42)\n",
    "    \n",
    "    results = []\n",
    "    counter = 1\n",
    "\n",
    "    max_f1 = 0\n",
    "    \n",
    "    for curr_test in tests:\n",
    "        \n",
    "        outliers_option, scaling, pca, resampling = curr_test\n",
    "        field_0 = ''\n",
    "        \n",
    "        pipe_steps = []\n",
    "      \n",
    "        if outliers_option == \"LOF\":\n",
    "            field_0 += 'LOF + '\n",
    "            pipe_steps.append((\"outlier\", FunctionSampler(func=outliers_lof)))\n",
    "        \n",
    "        if scaling:\n",
    "            field_0 += 'SCALE + '\n",
    "            pipe_steps.append((\"scale\",StandardScaler()))\n",
    "            \n",
    "        if pca:\n",
    "            field_0 += 'PCA + '\n",
    "            pipe_steps.append((\"PCA\",PCA(n_components=11, random_state=random_state)))\n",
    "          \n",
    "        if resampling:\n",
    "            field_0 += f'{resampling}'  \n",
    "            \n",
    "        if resampling == \"NearMiss\":\n",
    "            pipe_steps.append((resampling, NearMiss(version=3))) # 50-50\n",
    "        elif resampling == \"SMOTE\":\n",
    "            pipe_steps.append((resampling,  SMOTE(sampling_strategy=0.5, random_state=random_state) )) # 50-50\n",
    "        elif resampling == \"SMOTEEN\":\n",
    "            pipe_steps.append((resampling, SMOTEENN(0.77,random_state=random_state))) # 50.21 - 49.29\n",
    "            \n",
    "        pipe_steps.append(('clf',model))\n",
    "        \n",
    "        pipeline = imb_pipeline(pipe_steps)    \n",
    "        \n",
    "        gridsearch = GridSearchCV(pipeline, param_grid, scoring='f1', n_jobs=-1)\n",
    "        gridsearch.fit(X_train, y_train)\n",
    "        \n",
    "        best_estimator = gridsearch.best_estimator_\n",
    "        \n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        \n",
    "        f1 = round(f1_score(y_test, y_pred),2)\n",
    "        \n",
    "        if f1 > max_f1:\n",
    "            y_pred_max = y_pred\n",
    "            y_test_max = y_test\n",
    "            max_f1 = f1\n",
    "            \n",
    "        # clean field_0 > remove + if there is no more text\n",
    "        if field_0.split(\"+\")[-1] == ' ':\n",
    "            field_0 = field_0[:-3]\n",
    "            \n",
    "        curr_result = (field_0, f1, gridsearch.best_params_)\n",
    "        results.append(curr_result)\n",
    "        \n",
    "        print(f\"Done {counter}/{len(tests)} \\t | recall : {f1} | >> {field_0}\")\n",
    "        counter += 1\n",
    "\n",
    "    return results, y_pred_max, y_test_max\n",
    "\n",
    "def plot_summary(result,y_pred, y_test):\n",
    "    \"\"\"Plots a barchart with the partial results and a confusion matrix\n",
    "    on the best estimator with the best preprocessing technique\"\"\"\n",
    "    \n",
    "    fig = plt.figure(constrained_layout=False,figsize=(14,4))\n",
    "    gs1 = fig.add_gridspec(nrows=1, ncols=6, left=0.1, right=0.90, wspace=0.5, hspace=0.5)\n",
    "    ax1 = fig.add_subplot(gs1[0,0:3])\n",
    "    ax2 = fig.add_subplot(gs1[0,3:5])\n",
    "    \n",
    "    \n",
    "    plot_x = []\n",
    "    plot_y = []\n",
    "    \n",
    "    for r in result:\n",
    "        plot_x.append(r[1])\n",
    "        plot_y.append(r[0])\n",
    "\n",
    "    temp_df = pd.DataFrame({'x':plot_x, 'y' :plot_y}).sort_values(['x'], ascending=False)\n",
    "    \n",
    "    sns.barplot(x='x',y='y',data=temp_df, palette='viridis',ax=ax1)\n",
    "    ax1.set_xlabel(\"F1-score\", fontsize=\"14\")\n",
    "    ax1.set_ylabel(\"\")\n",
    "    ax1.set_yticklabels(temp_df.y,fontsize=13)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot = True, \n",
    "                cmap = 'viridis', \n",
    "                annot_kws={\"size\": 14},\n",
    "                linecolor = 'w',\n",
    "                linewidth = 4,ax=ax2,\n",
    "                fmt='d'\n",
    "               )\n",
    "    \n",
    "    ax2.set_xlabel(\"Predicted labels\",fontsize=\"14\")\n",
    "    ax2.set_ylabel(\"True labels\",fontsize=\"14\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_options = [None]\n",
    "scaling = [True]\n",
    "outliers_options = [None, \"LOF\"]\n",
    "resampling_options = [None, \"NearMiss\", \"SMOTE\", \"SMOTEENN\"]\n",
    "\n",
    "tests = make_tests(outliers_options,scaling,pca_options,resampling_options)\n",
    "\n",
    "results_rf, y_pred_rf, y_test_rf = training_pipeline(df, tests, models[\"rf\"], param_grids[\"rf\"])\n",
    "\n",
    "plt_rf = plot_summary(results_rf, y_pred_rf, y_test_rf)\n",
    "# plt_rf.savefig('rf-results.svg')\n",
    "plt_rf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_options = [\"PCA\"]\n",
    "scaling = [True]\n",
    "outliers_options = [None, \"LOF\"]\n",
    "resampling_options = [None, \"NearMiss\", \"SMOTE\", \"SMOTEENN\"]\n",
    "\n",
    "tests = make_tests(outliers_options,scaling,pca_options,resampling_options)\n",
    "\n",
    "results_knn, y_pred_knn, y_test_knn = training_pipeline(df, tests, models[\"knn\"], param_grids[\"knn\"])\n",
    "\n",
    "plt_knn = plot_summary(results_knn, y_pred_knn, y_test_knn)\n",
    "# plt_knn.savefig('knn-results.svg')\n",
    "plt_knn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_options = [None]\n",
    "scaling = [True]\n",
    "outliers_options = [None, \"LOF\"]\n",
    "resampling_options = [None, \"NearMiss\", \"SMOTE\", \"SMOTEENN\"]\n",
    "\n",
    "tests = make_tests(outliers_options,scaling,pca_options,resampling_options)\n",
    "\n",
    "results_lr, y_pred_lr, y_test_lr = training_pipeline(df, tests, models[\"lr\"], param_grids[\"lr\"])\n",
    "\n",
    "plt_lr = plot_summary(results_lr, y_pred_lr, y_test_lr)\n",
    "# plt_lr.savefig('lr-results.svg')\n",
    "plt_lr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_options = [None\"]\n",
    "scaling = [True]\n",
    "outliers_options = [None, \"LOF\"]\n",
    "resampling_options = [None, \"NearMiss\", \"SMOTE\", \"SMOTEENN\"]\n",
    "\n",
    "tests = make_tests(outliers_options,scaling,pca_options,resampling_options)\n",
    "\n",
    "results_svm, y_pred_svm, y_test_svm = training_pipeline(df, tests, models[\"svc\"], param_grids[\"svc\"])\n",
    "\n",
    "plt_svm = plot_summary(results_svm, y_pred_svm, y_test_svm)\n",
    "# plt_svm.savefig('svm-results.svg')\n",
    "plt_svm.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
